[
  {
    "objectID": "day1_exercises.html",
    "href": "day1_exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "The data for this exercise are taken from Van De Schoot et al. (2010). The study examined the understanding of antisocial behaviour and its association with popularity and sociometric status in a sample of at-risk adolescents from diverse ethnic backgrounds (\\(n = 1491\\), average age 14.7 years). Both overt and covert types of antisocial behaviour were used to distinguish subgroups. For the current exercise you will carry out a regression analysis where you want to predict levels of socially desirable answering patterns of adolescents (sw) using the predictors overt (overt) and covert antisocial behaviour (covert).\nBefore you will carry out this regression analysis in Mplus, you will first conduct the analysis in a program of your own choice. By first conducting the analysis in another program than Mplus, you can later check whether the results of Mplus are comparable. If they are similar, you know you specified everything correctly in Mplus.\nOpen the SPSS file popular_regr_1.sav (or popular_regr_1.xlsx or popular_regr_1.txt if you are working with different software). Note that any other software can be used. Also, note that the aim of the exercise is not to learn how to work with SPSS.\n\n\nAsk for descriptive statistics and run a correlation analysis in SPSS (or any other program of your own choice) between the variables of interest. What do you think of the correlations (significance, direction, magnitude)? You can use the following SPSS syntax:\nDESCRIPTIVES VARIABLES=sw covert overt\n /STATISTICS=MEAN STDDEV MIN MAX.\nCORRELATIONS\n /VARIABLES=sw covert overt\n /PRINT=TWOTAIL NOSIG\n /MISSING=PAIRWISE.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe descriptive statistics and correlations are shown below:\n\n\n\n\n\n\n\nRun a regression analysis. The dependent variable is sw and the independent variables are covert and overt. You can use the following SPSS syntax:\nREGRESSION\n /MISSING LISTWISE\n /STATISTICS COEFF OUTS R\n /CRITERIA=PIN(.05) POUT(.10)\n /NOORIGIN\n /DEPENDENT sw\n /METHOD=ENTER covert overt\n /SCATTERPLOT=(sw , covert) (sw , overt).\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe descriptive results of the regression analysis are shown below:\n\n\n\n\n\n\n\nNow, prepare the data for Mplus analyses:\n\nIn SPSS, recode all user and system missings values into -999. To do this, go to: Transform \\(\\rightarrow\\) Recode into same variables \\(\\rightarrow\\) select all variable and put these in the Variable box \\(\\rightarrow\\) Old and New values \\(\\rightarrow\\) select ‘System or User missing’ and enter the value -999 \\(\\rightarrow\\) Add \\(\\rightarrow\\) Continue \\(\\rightarrow\\) OK. Alternatively, you could use the following syntax:\n\nRECODE\nrespnr Dutch gender sw covert overt\n(missing=-999) (else=copy).\nEXECUTE.\n\nSave your data as a tab-delimited file (e.g., data_regr.dat) without variable names. There are 2 ways to save your data: through the menu ‘save as’ (see lecture slides - do not forget to turn off the selection ‘write variable names to spreadsheet’), or through SPSS syntax. An example of syntax is given below:\n\nSAVE TRANSLATE OUTFILE='C:\\directory\\your.filename.dat'\n/TYPE=TAB\n/textoptions decimal=dot\n/MAP\n/REPLACE\n/CELLS=VALUES.\n\nOpen Mplus, write the syntax file, and ask only for the sample statistics (you can copy all variable names from the SPSS file, that will save you some time with larger data sets [go to ‘variable view’, select all variable names and use Crtl + c in SPSS and Crtl + v in Mplus]). The Mplus syntax file should look like this:\n\nDATA:         \n  FILE = yourfilename.dat;\n  \nVARIABLE:\n  NAMES = respnr Dutch gender sw covert overt;\n  USEVARIABLES = covert sw overt;\n\nOUTPUT: \n  SAMPSTAT;\n\nSave your input file in the same folder that contains your data file. Go through the entire output file and find the sample statistics (sample size, means and correlations). Are there differences with the results of the SPSS analyses?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nRequesting the sample statistics without specifying any model does not only give information about the data, sample size, covariance & correlation matrix etc. It also returns model results, which might be unexpected. This “model” is simply the independence model where all variables are completely uncorrelated with all other variables – of course a very unrealistic, ill-fitting model.\nThe important results are displayed below:\n\nClearly there are big differences when compared with the results obtained in SPSS. The reason is that we “forgot” to tell Mplus -999 is used to denote missing data. Mplus treats the value -999 as an observed value.\n\n\n\n\n\n\nIn the previous run we forgot to include the missing data statement. So, add this statement to your syntax:\nDATA: \n  FILE = yourfilename.dat;\n  \nVARIABLE:\n  NAMES = respnr Dutch gender sw covert overt;\n  USEVARIABLES = covert sw overt;\n  MISSING = ALL (-999);\n  \nOUTPUT: sampstat;\nCompare the result again. Are there any differences in terms of sample statistics?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe important output is shown below:\n The sample means and correlations that Mplus returns are the same as those calculated in SPSS. There is a difference in reported sample size. In SPSS this is 1343 (see the correlation table) and in Mplus this is 1346. This is caused by missing data. If you inspect the following Mplus output, it becomes clear this is caused by missing data for the variables covert and overt:\n\n\n\n\n\n\n\nNow, add the model statements. (IVs = overt and covert, DV = sw) and also ask for standardized results. The syntax file should look like this:\nDATA: \n  FILE = your.filename.dat;\n  \nVARIABLE:\n  NAMES = respnr Dutch gender sw covert overt;\n  USEVARIABLES = covert sw overt;\n  MISSING = ALL (-999);\n  \nMODEL: \n  sw ON covert overt;\n  \nOUTPUT: \n  sampstat; STAND (stdyx);\n\n\n\n\n\n\nNote\n\n\n\nIn the model statement you specify sw (dependent variable) ON covert overt (independent variables). This might be counterintuitive, since we are used to specifying the independent variables before the dependent ones.\n\n\nInterpret the warning messages.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nMplus gives the following warnings:\n*** WARNING \n Data set contains cases with missing on all variables.\n These cases were not included in the analysis.\n Number of cases with missing on all variables: 145\n \n*** WARNING \n Data set contains cases with missing on x-variables.\n These cases were not included in the analysis.\n Number of cases with missing on x-variables: 3\n 2 WARNING(S) FOUND IN THE INPUT INSTRUCTIONS\n \nNumber of observations                                1343\nAs becomes clear when inspecting the total sample size used for the analyses (\\(n=1343\\)) Mplus used listwise deletion and deleted 145 cases because of missingness on the DV and three cases because of missingness on the IVs.\n\n\n\n\n\n\nNow, activate FIML. The following Mplus syntax can be used:\nDATA: \n  FILE = your.filename.dat;\n  \nVARIABLE:\n  NAMES = respnr Dutch gender sw covert overt;\n  USEVARIABLES = covert sw overt;\n  MISSING = ALL (-999);\n  \nMODEL:  sw ON covert overt;\n        covert overt;\n        \nOUTPUT: \n  SAMPSTAT; STAND (stdyx);\nInterpret the output and compare the results with the results obtained in SPSS.\n\nNote that the regression results for Exercise 1F are slightly different when compared with SPSS, but the results obtained in Exercise 1E are not. Why is this the case?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nNote the sample size used for the regression analysis:\nNumber of observations                                1346\nThe three missing cases have now been used in the analyses. This difference in sample size is causing minor difference in the output when comparing the results from SPSS (see above) and the results obtained in Mplus (see the output file multiple_regression with fiml.out)."
  },
  {
    "objectID": "day1_exercises.html#getting-data-into-mplus",
    "href": "day1_exercises.html#getting-data-into-mplus",
    "title": "Exercises",
    "section": "",
    "text": "The data for this exercise are taken from Van De Schoot et al. (2010). The study examined the understanding of antisocial behaviour and its association with popularity and sociometric status in a sample of at-risk adolescents from diverse ethnic backgrounds (\\(n = 1491\\), average age 14.7 years). Both overt and covert types of antisocial behaviour were used to distinguish subgroups. For the current exercise you will carry out a regression analysis where you want to predict levels of socially desirable answering patterns of adolescents (sw) using the predictors overt (overt) and covert antisocial behaviour (covert).\nBefore you will carry out this regression analysis in Mplus, you will first conduct the analysis in a program of your own choice. By first conducting the analysis in another program than Mplus, you can later check whether the results of Mplus are comparable. If they are similar, you know you specified everything correctly in Mplus.\nOpen the SPSS file popular_regr_1.sav (or popular_regr_1.xlsx or popular_regr_1.txt if you are working with different software). Note that any other software can be used. Also, note that the aim of the exercise is not to learn how to work with SPSS.\n\n\nAsk for descriptive statistics and run a correlation analysis in SPSS (or any other program of your own choice) between the variables of interest. What do you think of the correlations (significance, direction, magnitude)? You can use the following SPSS syntax:\nDESCRIPTIVES VARIABLES=sw covert overt\n /STATISTICS=MEAN STDDEV MIN MAX.\nCORRELATIONS\n /VARIABLES=sw covert overt\n /PRINT=TWOTAIL NOSIG\n /MISSING=PAIRWISE.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe descriptive statistics and correlations are shown below:\n\n\n\n\n\n\n\nRun a regression analysis. The dependent variable is sw and the independent variables are covert and overt. You can use the following SPSS syntax:\nREGRESSION\n /MISSING LISTWISE\n /STATISTICS COEFF OUTS R\n /CRITERIA=PIN(.05) POUT(.10)\n /NOORIGIN\n /DEPENDENT sw\n /METHOD=ENTER covert overt\n /SCATTERPLOT=(sw , covert) (sw , overt).\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe descriptive results of the regression analysis are shown below:\n\n\n\n\n\n\n\nNow, prepare the data for Mplus analyses:\n\nIn SPSS, recode all user and system missings values into -999. To do this, go to: Transform \\(\\rightarrow\\) Recode into same variables \\(\\rightarrow\\) select all variable and put these in the Variable box \\(\\rightarrow\\) Old and New values \\(\\rightarrow\\) select ‘System or User missing’ and enter the value -999 \\(\\rightarrow\\) Add \\(\\rightarrow\\) Continue \\(\\rightarrow\\) OK. Alternatively, you could use the following syntax:\n\nRECODE\nrespnr Dutch gender sw covert overt\n(missing=-999) (else=copy).\nEXECUTE.\n\nSave your data as a tab-delimited file (e.g., data_regr.dat) without variable names. There are 2 ways to save your data: through the menu ‘save as’ (see lecture slides - do not forget to turn off the selection ‘write variable names to spreadsheet’), or through SPSS syntax. An example of syntax is given below:\n\nSAVE TRANSLATE OUTFILE='C:\\directory\\your.filename.dat'\n/TYPE=TAB\n/textoptions decimal=dot\n/MAP\n/REPLACE\n/CELLS=VALUES.\n\nOpen Mplus, write the syntax file, and ask only for the sample statistics (you can copy all variable names from the SPSS file, that will save you some time with larger data sets [go to ‘variable view’, select all variable names and use Crtl + c in SPSS and Crtl + v in Mplus]). The Mplus syntax file should look like this:\n\nDATA:         \n  FILE = yourfilename.dat;\n  \nVARIABLE:\n  NAMES = respnr Dutch gender sw covert overt;\n  USEVARIABLES = covert sw overt;\n\nOUTPUT: \n  SAMPSTAT;\n\nSave your input file in the same folder that contains your data file. Go through the entire output file and find the sample statistics (sample size, means and correlations). Are there differences with the results of the SPSS analyses?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nRequesting the sample statistics without specifying any model does not only give information about the data, sample size, covariance & correlation matrix etc. It also returns model results, which might be unexpected. This “model” is simply the independence model where all variables are completely uncorrelated with all other variables – of course a very unrealistic, ill-fitting model.\nThe important results are displayed below:\n\nClearly there are big differences when compared with the results obtained in SPSS. The reason is that we “forgot” to tell Mplus -999 is used to denote missing data. Mplus treats the value -999 as an observed value.\n\n\n\n\n\n\nIn the previous run we forgot to include the missing data statement. So, add this statement to your syntax:\nDATA: \n  FILE = yourfilename.dat;\n  \nVARIABLE:\n  NAMES = respnr Dutch gender sw covert overt;\n  USEVARIABLES = covert sw overt;\n  MISSING = ALL (-999);\n  \nOUTPUT: sampstat;\nCompare the result again. Are there any differences in terms of sample statistics?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe important output is shown below:\n The sample means and correlations that Mplus returns are the same as those calculated in SPSS. There is a difference in reported sample size. In SPSS this is 1343 (see the correlation table) and in Mplus this is 1346. This is caused by missing data. If you inspect the following Mplus output, it becomes clear this is caused by missing data for the variables covert and overt:\n\n\n\n\n\n\n\nNow, add the model statements. (IVs = overt and covert, DV = sw) and also ask for standardized results. The syntax file should look like this:\nDATA: \n  FILE = your.filename.dat;\n  \nVARIABLE:\n  NAMES = respnr Dutch gender sw covert overt;\n  USEVARIABLES = covert sw overt;\n  MISSING = ALL (-999);\n  \nMODEL: \n  sw ON covert overt;\n  \nOUTPUT: \n  sampstat; STAND (stdyx);\n\n\n\n\n\n\nNote\n\n\n\nIn the model statement you specify sw (dependent variable) ON covert overt (independent variables). This might be counterintuitive, since we are used to specifying the independent variables before the dependent ones.\n\n\nInterpret the warning messages.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nMplus gives the following warnings:\n*** WARNING \n Data set contains cases with missing on all variables.\n These cases were not included in the analysis.\n Number of cases with missing on all variables: 145\n \n*** WARNING \n Data set contains cases with missing on x-variables.\n These cases were not included in the analysis.\n Number of cases with missing on x-variables: 3\n 2 WARNING(S) FOUND IN THE INPUT INSTRUCTIONS\n \nNumber of observations                                1343\nAs becomes clear when inspecting the total sample size used for the analyses (\\(n=1343\\)) Mplus used listwise deletion and deleted 145 cases because of missingness on the DV and three cases because of missingness on the IVs.\n\n\n\n\n\n\nNow, activate FIML. The following Mplus syntax can be used:\nDATA: \n  FILE = your.filename.dat;\n  \nVARIABLE:\n  NAMES = respnr Dutch gender sw covert overt;\n  USEVARIABLES = covert sw overt;\n  MISSING = ALL (-999);\n  \nMODEL:  sw ON covert overt;\n        covert overt;\n        \nOUTPUT: \n  SAMPSTAT; STAND (stdyx);\nInterpret the output and compare the results with the results obtained in SPSS.\n\nNote that the regression results for Exercise 1F are slightly different when compared with SPSS, but the results obtained in Exercise 1E are not. Why is this the case?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nNote the sample size used for the regression analysis:\nNumber of observations                                1346\nThe three missing cases have now been used in the analyses. This difference in sample size is causing minor difference in the output when comparing the results from SPSS (see above) and the results obtained in Mplus (see the output file multiple_regression with fiml.out)."
  },
  {
    "objectID": "day1_exercises.html#regression-in-mplus",
    "href": "day1_exercises.html#regression-in-mplus",
    "title": "Exercises",
    "section": "Regression in Mplus",
    "text": "Regression in Mplus\nIn this exercise we make use of a part (the Child Development Supplement) of the Panel Study of Income Dynamics (PSID). We would like to predict whether three dependent variables, Applied problems (APst02), Behavioral problems (problems), and Self esteem (selfesteem), can be predicted from two independent variables: letters words (LWst02), and digit span (DS02). Use the data file CDSsummerschool.sav.\n\nExercise 2A\nIt is a good exercise to draw the model we plan to estimate for yourself. Be very precise in how you draw the correlations.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\n\n\n\nExercise 2B\nRequest for descriptive statistics and correlations in SPSS. Then, prepare the SPSS data file for Mplus (do not forget to recode the missing values and save the data as a .dat file). Create the Mplus syntax file to analyze the research question.\nCheck if the descriptive statistics obtained in SPSS and Mplus are comparable. Think about the warnings about sample size. Do these make sense? Did you activate FIML?\nInterpret the output of Mplus. What are your conclusions? (Notice that you may get the warning that Selfesteem contains more than 8 characters. You can ignore this warning).\n\n\n\n\n\n\nNote\n\n\n\nIf you get an error message, check whether you specified which variables to use in this analysis. Genchild should not be used (If you do specify Genchild in the use variables statement, Mplus will automatically include it in parts of the model. Subsequently, it will complain that it is dichotomous but used as if continuous). Also, check whether you specified the missing values in the SPSS file before running your analysis (all missing values should be indicated by -999).\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFor the model statement in the input file you can use either:\nMODEL:\n  APst02 ON DS02 LWst02;\n  Problems ON DS02 LWst02;\n  Selfesteem ON DS02 LWst02;\nOr, alternatively, you can use:\nMODEL:\n  APst02 Problems Selfesteem ON DS02 LWst02;\nThe result is exactly the same. The model results can be found in ex2 CDS summerschool.out.\nNote that the zero chi-square implies that the model is fully saturated, meaning that the theoretical covariance matrix is a function of as many parameters as there are variances and covariances (on one side of the main diagonal) in the covariance matrix. You have essentially re-interpreted the covariance matrix, and this may be quite meaningful. Then, what you have is essentially a regression model, and just must evaluate your model in those terms. Are the estimated relationships strong? Are the direction of the parameter estimates consistent with theory? Are any assumptions violated (note that this would be checked before analysis)? Any basic regression text can guide you."
  },
  {
    "objectID": "day1_exercises.html#tech1-output",
    "href": "day1_exercises.html#tech1-output",
    "title": "Exercises",
    "section": "TECH1 output",
    "text": "TECH1 output\nThis exercise illustrates that communicating with Mplus sometimes can be difficult, because the TECH1 output can in some instances be misleading. This may confuse beginners (it confused us) but on the other hand, it is a useful reminder always to double check the User Guide and the other parts of the output as well.\nThe data for this exercise is about corporal punishment, which can be defined as the deliberate infliction of pain as retribution for an offence, or for the purpose of disciplining or reforming a wrongdoer or to change an undesirable attitude or behavior. Here we are interested in how corporal punishment influences children’s psychological maladjustment. Data come from 175 children between the ages of 8 and 18. The Physical Punishment Questionnaire (PPQ) was used to measure the level of physical punishment that was experienced.\nIn this exercise we focus on predicting psychological maladjustment (higher score implies more problems) by perceived rejection (e.g., my mother does not really love me; my mother ignores me as long as I do nothing to bother her; my mother goes out of her way to hurt my feelings). Moreover, rejection is predicted by perceived harshness (0 = never punished physically in any way; 16 = punished more than 12 times a week, very hard) and perceived justness (2 = very unfair and almost never deserved; 8 = very fair and almost always deserved).\nThe data consists of a covariance matrix taken from a published paper and can be found in the file CorPun.dat. So, besides analyzing a data set you collected yourself, in Mplus it is possible to base your analyses on a covariance or correlation table (and this is the reason why reviewers always ask you to include it).\n\nExercise 3A\nMake a drawing of the statistical model about corporal punishment and write down which parameters (e.g. regression paths, covariances, residuals) you expect that are estimated. Number these parameters.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nThe parameters you expect to be estimated are the following:\n\n2 variances (harsh, just)\n1 covariance (harsh with just)\n3 regression paths\n2 residual variances (reject and maladj)\n2 means (just and harsh, if means are available in the data)\n2 intercepts (reject and maladj, if means are available in the data)\n\nThis means we estimate 8 parameters in total excluding the means, or 12 including the means.\n\n\n\n\n\nExercise 3B\nWrite your Mplus input file for this model. Note that since you are using a covariance matrix as the input file, you should indicate this also in the input file, as well as the number of observations (See User’s Guide Examples 13.1 and 13.2). This can be done through:\nDATA:     FILE = CorPun.dat;       ! name of the file\n          TYPE = COVA;             ! indicate it is a covariance matrix\n          NOBSERVATIONS = 175;    ! indicate the number of cases\n          \nVARIABLE: NAMES = harsh just reject maladj;\nNow specify the model part yourself based on the drawing you made for exercise 3A.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe model command should look as follows:\nMODEL:    maladj ON reject;\n          reject ON harsh just;\nThe entire Mplus input file can be found on SURFdrive.\n\n\n\n\n\nExercise 3C\nNow, also ask for TECH1 output in your Mplus input file. This can be done by adding TECH1 to the OUTPUT command.\nUnder the subheading TECHNICAL 1 OUTPUT you can find six matrices (NU, LAMBDA, THETA, ALPHA, BETA and PSI) with numbers counting up to the total number of parameters that Mplus has estimated. This way you can check whether Mplus analyzed the model you wanted (and you will discover this is not always the case).\nThe regression coefficient between rejection and harshness belongs in the Beta matrix. Write down in which matrix all the other parameters listed in Exercise 3A should be.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe regression coefficients belong in the Beta matrix, the variances and residual variances belong in the Psi matrix. The means and intercepts are not estimated because they are not available in the data.\n\n\n\n\n\nExercise 3D\nRun your model and inspect the TECH1 output to see whether all parameters are estimated. Did Mplus estimate all the parameters that you expected? Notice that you can also answer this question by means of the Mplus diagrammer (see today’s lecture slides). The diagram will show you all regression paths, (residual) variances, and covariances. Note that it never provides you with information about the means (NU & ALPHA matrices in TECH1). Go to diagram in the menu and click: view diagram.\nWrite down the chi-square statistic and its degrees of freedom from the MODEL FIT INFORMATION. Also inspect the model results.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIn the model results, you can see that the estimates for the variances and covariance are not given. TECH1 also gives the impression that the variances and covariance of harsh and just were not estimated. However, the Mplus User Guide says that by default all 3 are included! When there is no syntax line specifying the variances of harsh and just, and no line specifying their covariance, the Tech1 output indicates that these 3 parameters are not estimated. You can see that these parameters are not included by examining the Psi matrix of the TECH1 output. Inspection of the starting values learns that these 3 parameters are assigned starting values. So what happens when you don’t ask for the (co)variances, is that Mplus includes them anyway, by default, but does not report those parameters and does not treat them as ‘real’ estimated parameters. You can also see this when you include the variances and covariance of harsh and just explicitly, as we will do in Exercise 3E.\n\n\n\n\n\nExercise 3E\nSpecify the following line of code in your model statement:\nMODEL:\n[...]\nharsh with just;\nharsh just;\nDoes anything change in the model results when you include this statement? Does anything change in the TECH1 output when you include this statement? (why do you think this is the case?). Did anything change to your chi-square statistic and its degrees of freedom?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWhen you include the statements harsh; just; you will see in the model results that the variances and the covariance of harsh and just is included, and that these are also specified in the TECH1 output. When a line of code is included to estimate the covariance, the variances are also automatically included in the TECH1 table of estimated parameters. But for clarity’s sake, it may be best to include both the variances and the covariance explicitly. The more explicit your code is, the more certain you can be that Mplus is doing what you want.\nThe first model specified in b) is the shortest, and Mplus defaults are included, but may not be evident in the output. Looking at the Tech1 output, you will not see the variances of harsh and just, nor their covariance. Neither are these specified in the results of the analysis. When you include the statements harsh; just; you will see in the model results that the variances and the covariance of harsh and just is included, and that these are also specified in the Tech1 output.\nIncluding the final statement harsh with just; does not change the tech1 output, or the model results, but it is the most specific way of specifying what happens in Mplus. The more specific you are in your input file, the more control you as a user have in telling Mplus what to do.\nBy adding the statement harsh with just; you know exactly what Mplus should do, and you can tell Mplus not to include this relationship by specifying harsh with just@0;\n\n\n\n\n\nExercise 3F\nSo how do we omit, for example, the covariance, by overriding the Mplus default? Stated otherwise, how do we make sure that the covariance of harsh and just is really NOT estimated (i.e., that it is set at 0)? To do this, we can use the following syntax:\nharsh WITH just@0; !where @0 implies the covariance is forced to be zero.\nInspect the number of degrees of freedom, model results and TECH1, what do you conclude?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThis example shows that the TECH1 output can be very useful to see what Mplus is doing, but it can also be misleading. Always cross check that Mplus has understood what you wanted it to do by looking at the model degrees of freedom, the TECH1 output / diagram and model results. When these pieces of information do not contradict each other and Mplus gives you exactly the output you expect, then you can move forward with interpreting the model effect. Also check the Mplus User Guide to be sure, and/or state everything that you want - and do NOT want –in your model very explicitly in your syntax. This way you have more certainty that Mplus is including and omitting exactly what you want, and not what it does in the background ‘by default’!"
  },
  {
    "objectID": "day1_introduction.html",
    "href": "day1_introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "The first exercise of today makes an explicit comparison on how certain analyses look in the software you’re used to (e.g., SPSS, Excel, SAS, STATA, R) and how the corresponding analyses look in Mplus. The aim is to understand the similarities between software packages, and the extensions that Mplus offers. Most exercises include SPSS syntax and Mplus setup. Feel free to use the windows interface in SPSS, or your own software.\nThe second exercise is aimed at doing a multivariate multiple regression analysis yourself in Mplus and to compose a .dat file suited for usage in Mplus.\nThe third exercise will make you familiar with the TECH1 output and shows that communicating with Mplus can be confusing and misleading sometimes.\nAll the data you need, as well as output files, can be found in SURFdrive. Make sure to unzip the files on your computer."
  },
  {
    "objectID": "day2_exercises.html",
    "href": "day2_exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "The file Sapi.dat contains the data that were used to conduct the EFA and CFA as presented in the lecture this morning. Try to replicate the following input files and/or results of today’s presentation and see what happens to the results. To prevent issues, make sure you specify in each of the to be replicated slides the value for missing data: MISSING ARE ALL (-999). Always double check by looking in the data set.\n\n\nOn slide 10, the input for the extraversion model is shown. Try to replicate the results for this model.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe correct MODEL command for the confirmatory factor model is displayed on slide 10. The correct factor loadings for this model are shown on slide 11. The entire input and output files can be found on the SURFdrive.\n\n\n\n\n\n\nOn slide 12, a Wald test is used to test whether two factor loadings can be considered equal. Include this test in your model and interpret the results.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe correct MODEL command for the Wald test is displayed on slide 12. The results of the Wald test are shown on slide 13. The entire input and output files can be found on the SURFdrive.\n\n\n\n\n\n\nCheck the output of the Confirmatory Factor model of Exercise A: Was the latent variable scaled via the variance of the factor (fixed factor scaling), or via the factor loadings (marker variable scaling)?\nAdjust the model so that you scale in the other way. That is, if Exercise 1A was scaled by fixing a factor loading to 1, then free this factor loading and fix the factor variance to 1. If part a was scaled by fixing the factor variance to 1, free the factor variance, and fix one of the factor loadings to 1.\nAs a bonus, scale via the factor loadings again, but now fix another factor loading to 1 than before, and free the factor loading that was previously fixed to 1. Based on your results, fill out the table below. What do you notice?\n\n\n\n\n\n\n\n\n\n\n\n\nScaling Method\nChi-square, df, p-value\nFactor loading Q77\nFactor loading Q84\nFactor loading Q170\nFactor loading Q196\nFactor variance\n\n\n\n\nReference Group\n\n\n\n\n\n\n\n\nMarker Variable\n\n\n\n\n\n\n\n\nMarker Variable Bonus\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScaling Method\nChi-square, df, p-value\nFactor loading Q77\nFactor loading Q84\nFactor loading Q170\nFactor loading Q196\nFactor variance\n\n\n\n\nReference Group\n57.075, 2, ~0.000\n0.835\n0.591\n0.473\n0.619\n1\n\n\nMarker Variable\n57.075, 2, ~0.000\n1\n0.708\n0.567\n0.742\n0.696\n\n\nMarker Variable Bonus\n57.075, 2, ~0.000\n1.347\n0.953\n0.764\n1\n0.384\n\n\n\nThe first thing you will notice is that all chi-square fit results are exactly the same across the models. This is because the different ways of scaling all result in equivalent statistical model fit.\nThe second thing you may notice is that the values of the loadings have changed. However, loadings that are relatively large (or small) in one model, are also relatively large (or small) in the other models. For example, in each model, the loading for Q77 is ~1.4 times as large as the loading of Q84.\nThe take home message is that model fit is not influenced by how you set scale. The parameter estimates change.\n\n\n\n\n\n\nOn slide 16, a CFA with categorical variables is specified. Try to replicate this and compare the conclusions of this analysis with the results when categorical variables are not specified. Important note: The categorical and continuous models cannot be formally compared with Chi-square tests or information criteria. So stick to your interpretations/conclusions when you compare the results.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe correct input is displayed on slide 16. The output can be found on slide 18. Note how the output now includes estimates of “thresholds”.\n\n\n\n\n\n\nSpecify an exploratory factor model (EFA), as was shown on slide 24-25.\n\n\n\n\n\n\nNote\n\n\n\nIf you want to run this analysis in the Mplus demo version, remove the variables Q196 and Q98 from the USEVARIABLES command in the model specification.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe correct input is displayed on slide 25. The correct resulting factor loadings are displayed on slide 26. Furthermore, the correct model fit results are displayed on slide 29.\nIf you ran the analysis in the Mplus demo version(without variables Q196 and Q98), the correct output is shown below:\n           GEOMIN ROTATED LOADINGS (* significant at 5% level)\n                  1             2\n              ________      ________\n Q77            0.981*        0.000\n Q84            0.381*        0.245*\n Q170           0.208*        0.256*\n Q44           -0.162*        0.634*\n Q63            0.030         0.532*\n Q76            0.005         0.542*\n\n\n           GEOMIN FACTOR CORRELATIONS (* significant at 5% level)\n                  1             2\n              ________      ________\n      1         1.000\n      2         0.476*        1.000\n\nRMSEA (Root Mean Square Error Of Approximation)\n\n          Estimate                           0.000\n          90 Percent C.I.                    0.000  0.042\n          Probability RMSEA &lt;= .05           0.980\n\nCFI/TLI\n\n          CFI                                1.000\n          TLI                                1.004\n\n\n           GEOMIN FACTOR CORRELATIONS (* significant at 5% level)\n                  1             2\n              ________      ________\n      1         1.000\n      2         0.476*        1.000\n\n\n\n\n\n\nOn slide 33 a confirmatory factor analysis was performed, try to replicate this model as well.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe correct MODEL command is displayed on slide 33. The correct resulting model fit is displayed on slide 34. Furthermore, the factor loading results are displayed on slide 35."
  },
  {
    "objectID": "day2_exercises.html#replicating-sapi-results",
    "href": "day2_exercises.html#replicating-sapi-results",
    "title": "Exercises",
    "section": "",
    "text": "The file Sapi.dat contains the data that were used to conduct the EFA and CFA as presented in the lecture this morning. Try to replicate the following input files and/or results of today’s presentation and see what happens to the results. To prevent issues, make sure you specify in each of the to be replicated slides the value for missing data: MISSING ARE ALL (-999). Always double check by looking in the data set.\n\n\nOn slide 10, the input for the extraversion model is shown. Try to replicate the results for this model.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe correct MODEL command for the confirmatory factor model is displayed on slide 10. The correct factor loadings for this model are shown on slide 11. The entire input and output files can be found on the SURFdrive.\n\n\n\n\n\n\nOn slide 12, a Wald test is used to test whether two factor loadings can be considered equal. Include this test in your model and interpret the results.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe correct MODEL command for the Wald test is displayed on slide 12. The results of the Wald test are shown on slide 13. The entire input and output files can be found on the SURFdrive.\n\n\n\n\n\n\nCheck the output of the Confirmatory Factor model of Exercise A: Was the latent variable scaled via the variance of the factor (fixed factor scaling), or via the factor loadings (marker variable scaling)?\nAdjust the model so that you scale in the other way. That is, if Exercise 1A was scaled by fixing a factor loading to 1, then free this factor loading and fix the factor variance to 1. If part a was scaled by fixing the factor variance to 1, free the factor variance, and fix one of the factor loadings to 1.\nAs a bonus, scale via the factor loadings again, but now fix another factor loading to 1 than before, and free the factor loading that was previously fixed to 1. Based on your results, fill out the table below. What do you notice?\n\n\n\n\n\n\n\n\n\n\n\n\nScaling Method\nChi-square, df, p-value\nFactor loading Q77\nFactor loading Q84\nFactor loading Q170\nFactor loading Q196\nFactor variance\n\n\n\n\nReference Group\n\n\n\n\n\n\n\n\nMarker Variable\n\n\n\n\n\n\n\n\nMarker Variable Bonus\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScaling Method\nChi-square, df, p-value\nFactor loading Q77\nFactor loading Q84\nFactor loading Q170\nFactor loading Q196\nFactor variance\n\n\n\n\nReference Group\n57.075, 2, ~0.000\n0.835\n0.591\n0.473\n0.619\n1\n\n\nMarker Variable\n57.075, 2, ~0.000\n1\n0.708\n0.567\n0.742\n0.696\n\n\nMarker Variable Bonus\n57.075, 2, ~0.000\n1.347\n0.953\n0.764\n1\n0.384\n\n\n\nThe first thing you will notice is that all chi-square fit results are exactly the same across the models. This is because the different ways of scaling all result in equivalent statistical model fit.\nThe second thing you may notice is that the values of the loadings have changed. However, loadings that are relatively large (or small) in one model, are also relatively large (or small) in the other models. For example, in each model, the loading for Q77 is ~1.4 times as large as the loading of Q84.\nThe take home message is that model fit is not influenced by how you set scale. The parameter estimates change.\n\n\n\n\n\n\nOn slide 16, a CFA with categorical variables is specified. Try to replicate this and compare the conclusions of this analysis with the results when categorical variables are not specified. Important note: The categorical and continuous models cannot be formally compared with Chi-square tests or information criteria. So stick to your interpretations/conclusions when you compare the results.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe correct input is displayed on slide 16. The output can be found on slide 18. Note how the output now includes estimates of “thresholds”.\n\n\n\n\n\n\nSpecify an exploratory factor model (EFA), as was shown on slide 24-25.\n\n\n\n\n\n\nNote\n\n\n\nIf you want to run this analysis in the Mplus demo version, remove the variables Q196 and Q98 from the USEVARIABLES command in the model specification.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe correct input is displayed on slide 25. The correct resulting factor loadings are displayed on slide 26. Furthermore, the correct model fit results are displayed on slide 29.\nIf you ran the analysis in the Mplus demo version(without variables Q196 and Q98), the correct output is shown below:\n           GEOMIN ROTATED LOADINGS (* significant at 5% level)\n                  1             2\n              ________      ________\n Q77            0.981*        0.000\n Q84            0.381*        0.245*\n Q170           0.208*        0.256*\n Q44           -0.162*        0.634*\n Q63            0.030         0.532*\n Q76            0.005         0.542*\n\n\n           GEOMIN FACTOR CORRELATIONS (* significant at 5% level)\n                  1             2\n              ________      ________\n      1         1.000\n      2         0.476*        1.000\n\nRMSEA (Root Mean Square Error Of Approximation)\n\n          Estimate                           0.000\n          90 Percent C.I.                    0.000  0.042\n          Probability RMSEA &lt;= .05           0.980\n\nCFI/TLI\n\n          CFI                                1.000\n          TLI                                1.004\n\n\n           GEOMIN FACTOR CORRELATIONS (* significant at 5% level)\n                  1             2\n              ________      ________\n      1         1.000\n      2         0.476*        1.000\n\n\n\n\n\n\nOn slide 33 a confirmatory factor analysis was performed, try to replicate this model as well.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe correct MODEL command is displayed on slide 33. The correct resulting model fit is displayed on slide 34. Furthermore, the factor loading results are displayed on slide 35."
  },
  {
    "objectID": "day2_exercises.html#multigroup-regression",
    "href": "day2_exercises.html#multigroup-regression",
    "title": "Exercises",
    "section": "Multigroup regression",
    "text": "Multigroup regression\nIn this exercise we extend the regression analysis of yesterday’s analysis to a multigroup analysis. You want to predict levels of socially desirable answering patterns of adolescents (sw) by overt (overt) and covert antisocial behaviour (covert), for males and females. Use the data file popular_regr_1.dat. Always double check the missing data code by looking in the data set.\n\nExercise 2A\nCreate a new Mplus syntax for a multiple group comparison between males and females to answer the question whether there are differences in the regression coefficients between sw and overt.\nThe Mplus syntax should look as follows:\nDATA:   […]\n\nVARIABLE:\n  NAMES = respnr Dutch gender sw covert overt;\n  USEVARIABLES = sw covert overt;\n  MISSING = ALL (99); \n  GROUPING = gender (0 = male 1 = female);\n  \nMODEL:\n  sw ON covert overt;\n\nOUTPUT:\n  CINTERVAL STDYX;\nYou will now get results for males and females separately. Interpret all the output and compare the results for males and females. Are there differences between males and females? What do the confidence intervals indicate?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe effect of covert on sw appears to be stronger for females (-0.478) than males (-0.438) and reverse for the effect of overt (-0.161 for males compared to -0.112 for females). However, the confidence intervals for SW on Covert and Overt show a large overlap for males and females. Additionally, the regression coefficients for the males are included in the confidence intervals for the females and vice versa. With this in mind, in the next question we are going to test whether the differences between sw ON covert and se ON overt are statistically significant.\n\n\n\n\n\nExercise 2B\nIf you want to compute a significance test to test whether the regression coefficient between sw and covert is the same or not, you can use the following syntax:\nDATA:     […]\n\nVARIABLE: NAMES = respnr Dutch gender sw covert overt;\n          USEVARIABLES = sw covert overt;\n          MISSING = ALL (99); \n          GROUPING = gender (0 = male 1 = female);\n          \nMODEL:    sw ON covert overt;\n          model male:\n          sw ON covert (b1);\n          sw ON overt;\n          model female:\n          sw ON covert (b2);\n          sw ON overt;\n          \nMODEL TEST: b1 = b2;\n\nOUTPUT:\nIn this syntax, the model for males and females is given separately by adding MODEL MALE: and MODEL FEMALE:. The (b1) and (b2) means that a label is given to the regression parameters of interest.\nUsing the MODEL TEST command you can test whether these coefficients are significantly different from each other. Inspecting the Wald Test of Parameter Constraints in the model fit output of Mplus gives you the results of this equality test. You can also do the tests manually, as was done in the lecture today. The results are similar. What do you conclude about the equality of regression parameters b1 and b2?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nMplus gives the following result:\nWald Test of Parameter Constraints\n       \n  Value                             1.035\n  Degrees of Freedom                1\n  P-Value                           0.3090\nThe Wald test tests whether the constraint holds (so whether b1 and b2 are equivalent). This test is non-significant, so we conclude that there is no evidence for a difference among b1 and b2."
  },
  {
    "objectID": "day2_exercises.html#measurement-invariance",
    "href": "day2_exercises.html#measurement-invariance",
    "title": "Exercises",
    "section": "Measurement Invariance",
    "text": "Measurement Invariance\nIn this exercise you will evaluate measurement invariance of the Prolonged Grief Disorder scale. Use:\nDATA:       \n  FILE = PGDdata2.dat;\n  \nVARIABLE:   \n  MISSING = ALL (-999);\n  NAMES = Kin2 b1pss1 b2pss2 b3pss3 b4pss4 b5pss5;\nThe 5 items are taken from the Inventory of Complicated Grief [@boelen_prolonged_2010].\n\nYearning;\nPart of self died;\nDifficulty accepting the loss;\nAvoiding reminders of deceased;\nBitterness about the loss\n\n\nExercise 3A\nRun a one-factor confirmatory factor analysis on the data set ignoring the multigroup structure (USEVARIABLES ARE b1pss1 b2pss2 b3pss3 b4pss4 b5pss5;) using the default parameterization and answer the following questions:\n\nHow many subjects are there?\n\nHow about the fit of the model?\n\nWhich item has the weakest contribution to the latent factor (in terms of standardized factor loading)?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nThere are 571 subjects.\nUsing the rules of thumb provided in yesterday’s lecture slides, we can conclude that the fit of the model is acceptable (RMSEA &lt; .06, SRMR &lt; .08, CFI/TLI &gt; .95).\n\nItem 2 has the weakest contribution to the factor. Its standardized factor loading is 0.494, the factor explains 24.4% of its variance.\n\n\n\n\n\n\n\nExercise 3B\nRun a one-factor confirmatory factor analysis on the data set with the multigroup structure using the default specifications of Mplus: GROUPING IS Kin2 (1 = partner 0 = else);. See the slides for how to test for configural, metric and scalar invariance using the Mplus shortcut commands. Answer the following questions:\n\nHow many subjects are there per group?\nHow is the model fit in the configural, metric and scalar invariant models?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThere are 190 subjects in the group ELSE and 379 in the group PARTNER.\nThe configural model fits the data (\\(\\chi^{2} = 11.329\\), \\(p = .33\\)). From there on, you evaluate whether the more constrained model does not fit the data worse than the less constrained model. The statistics below show that the metric invariance model does not fit worse than the configural model, and after that, that the scalar model does not fit worse than respectively the configural and the metric model.\nIn SEM, we always prefer parsimony: the model with the most df. Thus, we prefer the scalar invariance model, where both factor loadings and intercepts are constrained . Since the latent variable in the scalar model means the same thing across the two groups, we can compare values on the latent variable.\n                                          Degrees of\nModels Compared              Chi-square    Freedom     P-value\n\nMetric against Configural         8.088     4       0.0884\nScalar against Configural        12.968     8       0.1130\nScalar against Metric             4.880     4         0.2998\n\n\n\n\n\nExercise 3C\nTry to add constraints such that also the residual variances are constrained to be the same for both groups. Did the model get significantly worse? Check for AIC, BIC differences and use the chi-square difference test (\\(\\Delta \\chi^{2}\\)) that was discussed yesterday.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nConstraining the residual variances can be achieved with the following syntax:\nMODEL partner:\n  b1pss1(a);\n  b2pss2(b); \n  b3pss3(c); \n  b4pss4(d); \n  b5pss5(e);\n\nMODEL else:\n  b1pss1(a);\n  b2pss2(b); \n  b3pss3(c); \n  b4pss4(d); \n  b5pss5(e);\nThis yields the following Chi-Square Test of Model Fit:\nValue                             34.565\nDegrees of Freedom                23\nP-Value                           0.0574\n\nAkaike (AIC)                    6485.343\nBayesian (BIC)                  6559.189\nThe scalar model (see Exercise 3B) yields the following statistics:\nInformation Criteria\n\n          Akaike (AIC)                    6485.075\n          Bayesian (BIC)                  6580.640\n\nChi-Square Test of Model Fit\n\n          Value                             24.297\n          Degrees of Freedom         18\n          P-Value                           0.1455\n\\(\\Delta \\chi^{2} (5) = 10.268\\), \\(p = .0622\\), so the model is not significantly worse. Residual variances are also equal. We always prefer a more parsimonious model (i.e., more df) when we compare two models. You can use http://www.fourmilab.ch/rpkp/experiments/analysis/chiCalc.html to obtain the p-value for the \\(\\Delta \\chi^{2}\\)-test."
  },
  {
    "objectID": "day2_exercises.html#bonus",
    "href": "day2_exercises.html#bonus",
    "title": "Exercises",
    "section": "Bonus",
    "text": "Bonus\n\nExercise B1\nUsing the file popular_factor.dat run an EFA that provides you with a one-factor and a two-factor solution. Interpret the results for the two-factor solution. The syntax file should look something like this:\nDATA:         FILE = your.filename.dat;\n\nVARIABLE:  \n  NAMES = c1-c3 o1-o3;\n  MISSING = ALL (99);\n\nANALYSIS:       TYPE = EFA 1 2;\n\nOUTPUT:       SAMPSTAT TECH1;\n\n\n\n\n\n\nNote\n\n\n\nIn TYPE = EFA 1 2;, the first number specifies the smallest amount of factors you are interested in, and the second number specifies the maximum amount of factors you are interested in. So, EFA 1 2 will provide you with a 1- and 2-factor solution. EFA 2 5 would provide you with a 2-, 3-, 4-, and 5-factor solution. EFA 2 2 would provide you with only a 2-factor solution.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFrom the output, we can conclude that the 2-factor structure fits well to the data (Chi-Square Test of Model Fit is not significant, CFI/TLI is &gt; 0.95, and RMSEA is &lt; 0.05).\n\n\n\n\n\nExercise B2\nNow, run a CFA in Mplus with one factor, and another model with two factors (use again popular_factor.dat). The input file for the 2-factor model should look like this:\nDATA:         FILE = your.filename.dat;\nVARIABLE: \n  NAMES = c1-c3 o1-o3;\n  MISSING = ALL (99);\n  \nMODEL:\n  covert by c1* c2 c3;\n  overt by o1* o2 o3;\n\n  [covert@0];\n  [overt@0];\n\n  covert@1;\n  overt@1; \n\nOUTPUT:       SAMPSTAT STAND TECH1;\nThe input file for the one-factor model should look like this:\nDATA:         FILE = your.filename.dat;\n\nVARIABLE: \n  NAMES = c1-c3 o1-o3;\n  MISSING = ALL (99);\n\nMODEL:\n  anti by c1* c2 c3 o1 o2 o3;\n\n  [anti@0];\n  anti@1; \n\nOUTPUT:       SAMPSTAT STAND TECH1;\n\n\n\n\n\n\nNote\n\n\n\nIn both models we have scaled in this model by fixing the factor variance, rather than fixing a loading.\n\n\nAnswer the following questions:\n\nWhich model is to be preferred? What can you say about the model fit based on the AIC and BIC?\nWhat is the correlation between the two factors?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nComparing the two models (1-, and 2-factor) by looking at the AIC/BIC, the 2-factor solution is preferred: The AIC/BIC of the 2-factor model is lower than the AIC/BIC of the 1-factor model.\nThe correlation between OVERT and COVERT is 0.431 with a standard error of 0.055. So, the proportion of shared variance is 0.4312 = 0.186 = 18,6%. Note that in the output above we find this under STANDARDIZED RESULS. Whether the unstandardized OVERT WITH COVERT can be interpreted as a correlation or a covariance, depends on how the latent variable was scaled. If the factor variances are fixed to 1 (scaling is done via the factor variances), then the unstandardized result reflects the factor correlation, otherwise they reflect the covariance.\n\n\n\n\n\nExercise B3\nRun a confirmatory factor analysis with two factors as in Exercise B2, but now define the items as categorical. Are there fundamental differences compared to the previous model where we assumed the items are continuous variables?\n\n\n\n\n\n\nImportant\n\n\n\nThe categorical and continuous models cannot be formally compared with Chi-square tests or information criteria. So stick to your interpretations/conclusions when you compare the results.\n\n\nDATA:        FILE = data_factor.dat;\n\nVARIABLE: \n  NAMES = c1-c3 o1-o3;\n  MISSING = ALL (99);\n  CATEGORICAL = all;\n\nMODEL:\n  covert by c1 c2 c3;\n  overt by o1 o2 o3;\n\nOUTPUT:     SAMPSTAT STAND TECH1; \n\n\n\n\n\n\nAnswer\n\n\n\n\n\nComparing the standardized estimates of the categorical and non-categorical results, we see (in this example) fairly similar results when we treat the variables as categorical rather than continuous. You can see that the loadings and factor correlations (the estimates) appear to be larger. We also see that the O2 variable loads relatively strong on the OVERT variable now compared to the model with continuous indicators.\nRemember: the decision to choose for categorical or non-categorical should be theory driven."
  },
  {
    "objectID": "day2_introduction.html",
    "href": "day2_introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "We covered a lot of material today, and plenty of exercises to match. In exercise 1, you replicate what we did in the lectures with respect to the basics of factor analysis. In exercise 2 and 3, we move on to multigroup regression and measurement invariance. At the end of the document, we have some bonus exercises on CFA and EFA."
  },
  {
    "objectID": "day3_part1.html",
    "href": "day3_part1.html",
    "title": "Part 1: Factor analysis and nested model check",
    "section": "",
    "text": "Suppose there are three researchers who are investigating the underlying factor structure of psychopathology (i.e., mental illness). The data (file: ex5.6.dat) they have obtained consists of 12 items, where:\n\nAnx1, Anx2, and Anx3 measure anxiety;\nDep1, Dep2, and Dep3 measure depression;\nAdd1, Add2, and Add3 measure addictive behaviors;\nCon1, Con2, and Con3 measure conflict behaviors.\n\nThe four researchers have different ideas about how these data should be modeled. Their ideas are depicted and described below.\nResearcher 1 assumes that there is one general psychopathology factor P, and that there are four unique (and uncorrelated) factors for the four different clusters of items. Such a model is known as the bi-factor model.\n\n\n\nThe bi-factor model.\n\n\nResearcher 2 assumes that the best way to describe these data is by use of a 4-factor model, in which these factors are allowed to covary.\n\n\n\nThe 4-factor model.\n\n\nResearcher 3 also believes there is a general factor P, but assumes this is a factor that relates the other four factors. This is known as a second-order factor model.\n\n\n\nThe second-order factor model.\n\n\nResearcher 4 thinks the approach should be simpler and assumes there are two factors: An internalizing factor (consisting of the anxiety and depression items), and an externalizing factor (consisting of the addiction and conflict items). This is the two-factor model.\n\n\n\nThe second-order factor model.\n\n\n\nExercise 1\nSpecify each of these models in Mplus, and add the missing information to the table below. You can use the number of free parameters and/or the number of df to check whether you specified the model correctly.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\nModel\nFree parameters\ndf\n\\(\\chi^{2}\\)\np-value\n\n\n\n\nbi-factor model\n48\n42\n41.64\n.487\n\n\n4-factor model\n42\n48\n45.78\n.564\n\n\nsecond-order factor model\n40\n50\n46.74\n.605\n\n\n2-factor model\n37\n53\n1780.55\n\\(&lt;\\).001\n\n\n\n\n\n\n\n\nExercise 2\nThe researchers want to compare the models using a \\(\\Delta \\chi^{2}\\)-test, if possible. Can you help these researchers figure out whether or not these models are nested? Note that a necessary but not sufficient condition for nesting is that the nested (more parsimonious model) has fewer free parameters (thus more df) than the more general model. Hence, based on the number of free parameters, we can make the following comparisons:\n\n\n\nMore general model\nMore parsimonious model(s)\nNested?\n\n\n\n\nbi-factor model\n4-factor model\n…\n\n\nbi-factor model\nsecond-order factor model\n…\n\n\nbi-factor model\n2-factor model\n…\n\n\n4-factor model\nsecond-order factor model\n…\n\n\n4-factor model\n2-factor model\n…\n\n\nsecond-order factor model\n2-factor model\n…\n\n\n\nPerform the nesting checks included in the table and indicate for each whether the models are nested. Use this information in the figure below to indicate for each arrow whether or not the models are nested.\n\n\n\n\n\n\n\nImportant: The SAVEDATA command\n\n\n\n\n\nTo compare the models, first run the most parsimonious model and save data for a nested test by adding to following command to your syntax:\nSAVEDATA: NESTED = 2factormodel.dat;\nIf you are using Mplus via SolisWorkspace, then you need to explicitly specify the path to which you want to save the 2factormodel.dat to for this command to work. Mplus will throw an error if you fail to do this. Specify a path within apostrophes (“ “) and do not forget to end the command with a semi-colon. You can find the path within Mplus by going to the File-menu, and clicking on ‘Open…’. Navigate to the folder where you want to save the .dat-file in, and copy the path in the navigation bar. Paste this path within the apostrophes in the SAVEDATA command, and extend with the name that you want to give the .dat-file.\nThe SAVEDATA-command should be something like:\nSAVEDATA: NESTED = \"\\\\Client\\C$\\Users\\...\n          …S20 - Day 3\\Exercise 1\\2factormodel.dat\";\nCommands in Mplus must not exceed 90 characters per line. So if your path is quite long, don’t forget to break up your command (before the 90\\(^{th}\\) character) and continue on a second (or third, etc.) line. See the example above. Then run the model that that is more general, and perform the nested test by adding (here you don’t need to explicitly specify the path to the .dat-file):\nANALYSIS: NESTED = 2factormodel.dat;\nThis gives you the following output under the fit statistics (just above the SRMR):\nNested Model Check\n\n          Result                        Not Nested\n          Fit Function Value            0.09146430\nThis implies that the 2-factor model of research 4 is not a special case of the second-order factor model of researcher 3.\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n Take home message:\n\nConclusion 1: Parsimony—while a necessary condition—is not a sufficient condition for nesting.\nConclusion 2: Models that look quite different, may nevertheless be nested."
  },
  {
    "objectID": "day3_part2.html",
    "href": "day3_part2.html",
    "title": "Part 2: Full SEM, with mediation and moderation",
    "section": "",
    "text": "Two researchers are interested in burnout among academics. They have obtained the data (file: Ex2sim.dat) from 300 academics on the following variables:\nThe researchers want to investigate whether the detrimental effect of Emotional Exhaustion on Academic Achievement is mediated by Cynicism. They also want to control for differences between men and women in Emotional Exhaustion, Cynicism, and Academic Achievement. Hence, they propose the following model:\nNote that, even though Fem is a categorical variable (with scores 0 or 1), we do not have to treat it differently in Mplus than the continuous variables because:"
  },
  {
    "objectID": "day3_part2.html#exercise-1",
    "href": "day3_part2.html#exercise-1",
    "title": "Part 2: Full SEM, with mediation and moderation",
    "section": "Exercise 1",
    "text": "Exercise 1\nSpecify this model and discuss the model fit.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSee the input file for Model 1. The \\(\\chi^{2}\\) test indicate that the model does not fit, \\(\\chi^{2} (16) = 33.06\\), \\(p = .0073\\). Other measures indicate acceptable model fit: RMSEA is 0.06, CFI is 0.96, TLI is 0.93, and SRMR is 0.04."
  },
  {
    "objectID": "day3_part2.html#exercise-2",
    "href": "day3_part2.html#exercise-2",
    "title": "Part 2: Full SEM, with mediation and moderation",
    "section": "Exercise 2",
    "text": "Exercise 2\nInvestigate whether there is an indirect (mediated) effect of Emotional Exhaustion on Academic Achievement. Remember to use the MODEL INDIRECT command, and combine this with bootstrapping and confidence intervals (see slides).\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSee the Ex2Model2.out file. To interpret the size of a (significant) effect, it is helpful to use the standardized results. When looking at the p-values for the standardized (bootstrap) results, we find:\n\nTotal effect is significant: -0.152 (SE = 0.067, \\(p = .023\\))\nIndirect effect is not significant: -0.071 (SE = 0.038, \\(p = .058\\))\nDirect effect is not significant: -0.081 (SE = 0.068, \\(p = .234\\))\n\nNote that the total effect is the sum of the indirect and the direct effects: \\(-0.152 = (-0.071) + (-0.081)\\). The results here show that, while we do not have the power to detect a direct or an indirect effect, the total effect is significant.\nThe 95% confidence intervals for the standardized effects are:\n\nTotal effect (-0.283,-0.021); does not contain zero\nIndirect effect (-0.145,0.002); contains zero\nDirect effect (-0.213,0.052); contains zero\n\nHence, the conclusions based on the confidence intervals are the same as the conclusions based on the p-values."
  },
  {
    "objectID": "day3_part2.html#exercise-3",
    "href": "day3_part2.html#exercise-3",
    "title": "Part 2: Full SEM, with mediation and moderation",
    "section": "Exercise 3",
    "text": "Exercise 3\nDo the same for the effect of being female on academic achievement.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nMoving to the confidence intervals for standardized parameters right away, we find:\n\nTotal effect is -0.020, 95% CI [-0.131, 0.090]; contains zero\nIndirect effect FEM \\(\\rightarrow\\) Cynicism \\(\\rightarrow\\) AcAch is 0.038, 95% CI [-0.014, 0.091]; contains zero\nIndirect effect FEM \\(\\rightarrow\\) EMEX \\(\\rightarrow\\) AcAch is -0.013, 95% CI [-0.038, 0.013]; contains zero\nIndirect effect FEM \\(\\rightarrow\\) EMEX \\(\\rightarrow\\) Cynicism \\(\\rightarrow\\) AcAch is -0.011, 95% CI [-0.028, 0.005]; contains zero\nDirect effect is -0.035, 95% CI [-0.153,0.084]; contains zero.\n\nConclusion: there is no evidence that being female has a direct or indirect effect on academic achievement."
  },
  {
    "objectID": "day3_part2.html#exercise-4",
    "href": "day3_part2.html#exercise-4",
    "title": "Part 2: Full SEM, with mediation and moderation",
    "section": "Exercise 4",
    "text": "Exercise 4\nThe researchers are also interested in investigating whether there is an interaction between being female and emotional exhaustion on cynicism; specifically, they want to know whether the effect of emotional exhaustion on cynicism is stronger for males than for females. The model they want to specify for this is visualized below:\n\nRemember that to specify an interaction between an observed and a latent variable (indicated here by the black dot) in Mplus, you need to use the XWITH statement in the MODEL command. The use of this statement requires you to use (see slides):\nANALYSIS:   TYPE = RANDOM; \n            ALGORITHM = INTEGRATION; \nNote that this cannot be combined with bootstrapping, or the MODEL INDIRECT command (you get error messages when you try to do this). The latter does not imply there are no indirect effects in the model; it merely implies you cannot get the additional output for this.\nSpecify this model in Mplus.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSee the iput file for Model 3. The interaction is specified as:\nEExF | EmEx XWITH Fem;      ! Interaction\nThis model gives\nCYNICISM   ON\n    EMEX               0.343      0.105      3.263      0.001\n    EEXF              -0.319      0.135     -2.361      0.018\nwhich implies that the interaction is significant. Substantively, it implies that:\n\nThe effect of Emotional Exhaustion for males (Fem = 0) is 0.343, SE = 0.105, \\(p &lt; .001\\).\nThe effect of Emotional Exhaustion for females (Fem = 1) is 0.319 lower than for men, SE = 0.135, \\(p = .018\\)."
  },
  {
    "objectID": "day3_part2.html#exercise-5",
    "href": "day3_part2.html#exercise-5",
    "title": "Part 2: Full SEM, with mediation and moderation",
    "section": "Exercise 5",
    "text": "Exercise 5\nWe now have a slope for males and a difference in slopes between males and females. If instead, we are interested in the actual slope of females, we can ask Mplus to compute this as an additional statistic using the MODEL CONSTRAINT command. To this end, we first need to name the slopes of interest, using\nCynicism ON EmEx Fem EExF (slopeM interDiff slopeDiff);  \nand subsequently we can use these names to compute new statistic, using\nMODEL CONSTRAINT:\n      new(slopeF);\n      slopeF = slopeM + slopeDiff;\nThis way we will obtain the estimate for the slope of females, including its standard error and p-value.\nSpecify this model in Mplus, and report the results.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSee the input file for Model 4. The slope for females (when regressing cynicism on emotional exhaustion is not significantly different from zero:\nNew/Additional Parameters\n    SLOPEF             0.024      0.103      0.233      0.816\nHence, we can conclude that, while for males emotional exhaustion is a positive predictor of cynicism (suggesting that more emotional exhaustion leads to higher levels of cynicism), for females, no such relation was found."
  },
  {
    "objectID": "day4_part1.html",
    "href": "day4_part1.html",
    "title": "Part 1: Investigating the mean structure, and CLPM",
    "section": "",
    "text": "In this first part of the computer lab you are going to analyze the same data that were used in the Hamaker, Kuiper, and Grasman (2015) paper and were presented in the lecture. The goals of these exercises are to get some experience with fitting (random intercept) cross-lagged panel models, interpreting their results, and comparing them. Here, we focus on the relationship between Parental Responsiveness and Adolescents’ Depressive Symptomatology (note, in the lecture the focus was on a different concept, namely Parental Psychological Control).\nThe data (means, standard deviations, and the correlation matrix) are included in the file Soenens.dat on SURFdrive. The number of observations is 396. The data command for these data should be:\nThe variable command should be:"
  },
  {
    "objectID": "day4_part1.html#exercise-1-mean-structure",
    "href": "day4_part1.html#exercise-1-mean-structure",
    "title": "Part 1: Investigating the mean structure, and CLPM",
    "section": "Exercise 1: Mean structure",
    "text": "Exercise 1: Mean structure\nSpecify a model in which you investigate whether the group means can be constrained across time. Tip: Allow all variables to covary with each other (i.e. specify a model without constraints on the covariance structure) such that we are estimating means, and not intercepts.\n\nExercise 1A\nDoes this model fit?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe model specification is given in S20_Day4_Exercise1A.inp on SURFdrive. The model does not fit well, according to the \\(\\chi^{2}\\) and the RMSEA; CFI and TLI indicate a good and barely acceptable model fit, respectively. See the slides of day 1 of this summer school if you need a refresher on the recommended cutoff values for these fit indices (although such cutoff recommendations remain a hotly debated topic among statisticians).\n\n\\(\\chi^{2}\\) (4) = 19.620, \\(p &lt; .001\\)\nRMSEA = 0.099\nCFI = 0.986\nTLI = 0.946\n\n\n\n\n\n\nExercise 1B\nWhat can you do to improve the model fit?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe can look at the modification indices in the output. Here, we specified MOD(4) in the OUTPUT command, which indicates all parameters that can be added/freed that will lead to a decrease in chi-square of at least 4 points (which is a significant improvement combined with 1 df).\nBased on the Mplus output, it appears that freeing the mean of Dep1 leads to the largest change in chi-square (M.I. = 10.53).\n\n\n\n\n\nExercise 1C\nRun the adjusted model and discuss the results.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSyntax for the adjusted model is in S20_Day4_Exercise1C.inp. Looking at the fit indices of the adjusted model, the chi-square for this model is still significant, but all other measures indicate the model fit is okay to excellent. Moreover, the model is a significant improvement in comparison to the previous model: \\(\\Delta\\chi^{2} = 19.62 - 8.71 = 10.91\\), with \\(\\Delta\\)df = 1 and, \\(p = .001\\).\nYou can calculate the p-value of the \\(\\Delta \\chi^{2}\\) using the pchisq()-function in R (with the lower.tail argument set to FALSE), or an online tool"
  },
  {
    "objectID": "day4_part1.html#exercise-2-the-clpm",
    "href": "day4_part1.html#exercise-2-the-clpm",
    "title": "Part 1: Investigating the mean structure, and CLPM",
    "section": "Exercise 2: The CLPM",
    "text": "Exercise 2: The CLPM\n\nExercise 2A\nSpecify the cross-lagged panel model (CLPM) in Mplus. Regress the observed variables directly on each other over time, as in the graph above. Try to think of where each parameter you estimate should go in the graph. Compare your model specification with the input file S20_Day4_Exercise2A.inp in SURFdrive.\n\n\n\nThe cross-lagged panel model.\n\n\n\n\nExercise 2B\nSpecify the model as in the graph below (the CLPM with centered latent variables), and again include the (significant) parameter estimates in this graph. Tip: Do not forget to constrain the measurement error variances of the observed variables to 0. Compare your model specification with the input file S20_Day4_Exercise2B.inp in SURFdrive.\n\n\n\nThe cross-lagged panel model with centered latent variables.\n\n\n\n\nExercise 2C\nThe two models should lead to the exact same model fit. Estimate the model and discuss the model fit. When comparing the parameter estimates, what is the difference between the model in exercise 2A and the model in exercise 2B?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe CLPM with centered latent variables is, just like the CLPM from exercise 2A, a traditional CLPM, but specified in such a way that the mean structure is first separated from the regression part of the model. This implies we model the means, rather than intercepts, which makes it easier to impose the constraint of identical means over time. So, the fit indices and most parameter estimates are equivalent, except for… (see next question)\n\n\n\n\n\nExercise 2D\nWhich parameter estimates differ? How can they be related?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe difference in parameter estimates is in the means/intercepts. For the first model we estimate the means at wave 1, and intercepts thereafter. For the second model we obtain mean estimates at each wave (although they are referred to as intercepts in the Mplus output).\nAgain, the reason for this is that in the first model, the means at wave 2 and 3 are partly predicted by the means from previous waves (through the lagged relationships). Therefore, the constants that are estimated should be interpreted as intercepts. In the second model, the mean part (triangles) and the regression part (lagged parameters) are separated.\n\n\n\n\n\nExercise 2E\nConstrain the means in the second version of the CLPM. Please do not constrain the mean of Dep1, however, as this constraint proved untenable in exercise 1C. Discuss the model fit.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe input for these models are in S20_Day4_Exercise2E.inp. The chi-square has increased in comparison with the previous model: The reason is that this is a restricted model (a special case of the previous model). We can test whether the increase in misfit is significant using the chi-square difference test: \\(\\Delta\\chi^{2} = 69.59 - 62.98 = 6.61\\) with \\(\\Delta\\)df = 7 – 4 = 3, \\(p = .0854\\). Hence, the constraints are tenable. We can also look at information criteria such as the AIC or BIC. The model with the lowest AIC or BIC is preferred.\n\n\n\n\n\nExercise 2F\nContinue with the model from the previous question and constrain all the lagged parameters between wave 1 and 2 to be identical to the lagged parameters between wave 2 and 3. Discuss the model fit, and compare this model to the previous one to see whether the constraint holds.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe input for these models are in S20_Day3_Exercise2F.inp. The chi-square has again increased in comparison with the previous model: The reason is that this is a restricted model (a special case of the previous model). We can test whether the increase in misfit is significant: \\(\\Delta\\chi^{2} 81.52 - 69.59 = 11.93\\), \\(\\Delta\\)(df) = 11 - 7 = 4, \\(p = .0178\\).\nIf we stick to an alpha of .05, the constraints must be rejected (as the increase in misfit is significant). Note that we are doing several model comparisons here, so one could also argue that we should use a smaller alpha to guard against an inflated overall type I error. Again, an alternative would be to use information criteria for model comparisons."
  },
  {
    "objectID": "day4_part2.html",
    "href": "day4_part2.html",
    "title": "Part 2: The RI-CLPM",
    "section": "",
    "text": "In the second part of the computer lab, we continue analyzing the data that were used in the Hamaker, Kuiper, and Grasman (2015) paper and were presented in the lecture. However, now we explore cross-lagged relations while controlling for stable, between-person differences."
  },
  {
    "objectID": "day4_part2.html#exericse-3-the-ri-clpm",
    "href": "day4_part2.html#exericse-3-the-ri-clpm",
    "title": "Part 2: The RI-CLPM",
    "section": "Exericse 3: The RI-CLPM",
    "text": "Exericse 3: The RI-CLPM\n\nExercise 3A\nSpecify the RI-CLPM for these data (without any constraints on the lagged parameters over time). Discuss the model fit.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe input for these models are in S20_Day4_Exercise3A.inp. This model fits very well; however, it only has 1 df, so it is not doing much for data reduction.\n\n\n\n\n\nExercise 3B\nSpecify the RI-CLPM with the constraints on the grand means at each occasions. Again, please do not constrain the mean of Dep1, however, as this constraint proved untenable in exercise 1C.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe input for these models are in S20_Day4_Exercise3B.inp. This model has an acceptable (RMSEA, \\(\\chi^{2}\\)) to good (CFI and TLI) fit. Comparing it to the previous model, we get a chi-square difference test of \\(\\Delta\\chi^{2} = 9.590 - 0.84 = 8.75\\), \\(\\Delta\\)df = 4 - 1 = 3, \\(p = .0328\\). This implies that the constraints we impose on the means are untenable (they lead to a significant worsening of model fit). You might therefore decide to ultimately not impose these constraints. However, for didactical reasons, we continue with these constraints in place in the next exercise.\n\n\n\n\n\nExercise 3C\nAdd the constraints on the lagged parameters to the previous model.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe input for these models are in S20_Day4_Exercise3C.inp. This model fits okay (depending on the measure). Comparing it to the previous model, we get \\(\\Delta\\chi^{2} = 19.74 - 9.59 = 8.15\\), \\(\\Delta\\)df = 8 - 4 = 4, \\(p = .0379\\). Again, this implies that constraining the lagged effects to be time-invariant is untenable (it leads to a significant worsening of model fit).\n\n\n\n\n\nExercise 3D\nCompare all the models so far simultaneously, using the AIC and BIC. Which model is selected?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBased on the AIC, the RI-CLPM without any constraints provides the closest fit to the data, followed by the two other RI-CLPMs (and the adjusted means model). Based on the BIC, the RI-CLPM with constraints on the means and the lagged parameters is superior to the other models.\n\n\n\nExercise\nInput file\nAIC\nBIC\n\n\n\n\n1\nexercise1A.inp\n9451\n9543\n\n\n3\nexercise1C.inp\n9442\n9538\n\n\n6\nexercise2A.inp and exercise2B.inp\n9494\n9586\n\n\n7\nexercise2E.inp\n9495\n9575\n\n\n8\nexercise2F.inp\n9499\n9563\n\n\n9\nexercise3A.inp\n9438\n9541\n\n\n10\nexercise3B.inp\n9441\n9533\n\n\n11\nexercise3C.inp\n9443\n9518"
  },
  {
    "objectID": "day4_part2.html#prepartion-consultation-day",
    "href": "day4_part2.html#prepartion-consultation-day",
    "title": "Part 2: The RI-CLPM",
    "section": "Prepartion consultation day",
    "text": "Prepartion consultation day\nOnce you have finished all the exercises, you can start preparing for the consultation day. Below are a couple of tips and questions that can help you in making the most out of this day.\n\nCan you translate your research question into a SEM diagram? A SEM diagram can greatly help communicating to others about your research question, and help you with setting up the model in Mplus.\nIn your SEM diagram, which arrows provide a direct answer to your research question (i.e., which arrows are of core interest)?\n\nDo you expect measurement error in your measurement, and has this been included in the path diagram?\nIn case of a longitudinal study design, how much time has passed between repeated measures? Does this time frame make theoretical sense (or is it too long/too short for you to capture your effect of interest)?\nAre there likely to be stable, between-person differences in your time-varying variables? Is this taken into account in the SEM diagram?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Structural Equation Modeling using Mplus",
    "section": "",
    "text": "These course materials are part of the course Introduction to Structural Equation Modeling using Mplus (S20). It is a five-day summer school course hosted by Utrecht University’s department of Methodology and Statistics. The main objective is to learn how to analyze several models with Mplus (e.g. path models, multiple group models, mediation and moderation, confirmatory factor analysis, and longitudinal models). More in-depth lectures on the fundamentals of Mplus and advanced longitudinal models are discussed in the advanced course (S23).\nUsing the menu on the left, you can navigate to the exercises of each day."
  },
  {
    "objectID": "index.html#help-and-resources-it-related-issues",
    "href": "index.html#help-and-resources-it-related-issues",
    "title": "Introduction to Structural Equation Modeling using Mplus",
    "section": "Help and Resources: IT-Related Issues",
    "text": "Help and Resources: IT-Related Issues\nThis video explains how you can start up Mplus using SolisWorkspace (SWS), and access files on your local computer. In case you run into IT-related issues, we have listed some useful resources below.\n\n\n\n\n\n\nSolisWorkspace\n\n\n\n\n\nIf you do not have Mplus installed locally on your computer, you will be able to use Mplus for the duration of the summer school via SolisWorkspace (SWS). For this, you need to install both SWS (including the Citrix Receiver app), and set up two-factor authentication (2FA):\nSWS\n\nYou can find the manuals for installing SWS on Windows, MacOS and Linux at https://manuals.uu.nl/en/a-z/?_categorie=applications&_tags=Solisworkspace.\nSolutions to common issues with SWS are described at https://manuals.uu.nl/en/manual/solisworkspace-faq/.\n\n2FA\n\nThe manual for setting up 2FA can be found at https://manuals.uu.nl/en/manual/twee-factor-authenticatie-faq/.\nSolutions to common issues with 2FA are described at https://manuals.uu.nl/en/manual/twee-factor-authenticatie-faq/(https://manuals.uu.nl/en/manual/twee-factor-authenticatie-faq/.\n\nHelp\nIf you need help installing SWS or setting up 2FA, please contact the UU IT Service Desk. They are present on day one of the summer school. Alternatively, you can reach them via\n\nthe IT service desk, to the right of the desk near the Educatorium,\nphone, by calling +31 30 253 4500,\nmail, at servicedesk@uu.nl, or\nor WhatsApp, at +31 6 10 03 44 93.\n\nPlease inform the IT service desk that this concerns an Utrecht Summer School course, and keep your SolisID ready.\nLog-In\nAfter you have installed the software, you can log into SWS using the steps below:\n\nGo to https://solisworkspace.uu.nl.\nLog in using SolisID@soliscom.uu.nl (and replace ‘SolisID’ with the login name you received from the Summerschool).\nConfirm login using 2FA.\n\n\n\n\n\n\n\n\n\n\n\nWiFi\n\n\n\n\n\nThere are two or three options for you to connect to WiFi:\n\nConnect to the Utrecht University-network using your SolisID. As a user name, use SolisID@soliscom.uu.nl, and then fill in the password for the Solis account that you got.\nIf you have Eduroam-access via your own research institute, you can use this also on the Utrecht Science Park.\nUse the Eduroam Visitor Access (EVA), the guest network of Eduroam. For this, you need a ‘day code’, which are depicted on screens in the hallway of UU-buildings. You should text this day code to the phone number also displayed on the screen. Then, you receive a login name and password. You then have WiFi for that entire day. Extra information can be found at https://manuals.uu.nl/a-z/?_tags=eduroam.\n\n\n\n\n\n\n\n\n\n\nPrinter\n\n\n\n\n\nWhen logged in on a UU-computer with a SolisID, you can print by sending the material to the printer and log in with the SolisID on the printer."
  },
  {
    "objectID": "index.html#help-and-resources-sem-analyses",
    "href": "index.html#help-and-resources-sem-analyses",
    "title": "Introduction to Structural Equation Modeling using Mplus",
    "section": "Help and Resources: SEM Analyses",
    "text": "Help and Resources: SEM Analyses\nSome resources about learning more about particular analysis in Mplus are:\n\nThe Mplus User Guide.\nThe Mplus Discussion Forum. Although it has been closed for new questions for some time now, you can still access the hundreds (if not thousands) of questions and answers. You can access the forum here, or simply google your analysis of interest and add “mplus forum” in the search prompt."
  }
]